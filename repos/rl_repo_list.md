# RL 레포 추천

## 선정 기준
- 환경 세팅과 실행 방법이 명확할 것
- 학습 곡선/로그가 확인 가능할 것
- 하이퍼파라미터 변경이 쉬울 것

## 주의 사항
- 해당 레포는 추천 자료일 뿐, 반드시 이 중에서 선택해야 하는 것은 아닙니다. 다른 레포를 찾아보거나, 논문 구현 레포를 직접 찾아보는 것도 좋은 방법입니다.
- 운영진은 추천 레포 리스트만 제공하며, 레포 선정과 실험 과정에서 발생하는 문제에 대해서는 지원하지 않습니다. 팀원들과 협력하여 문제를 해결해 나가는 것도 중요한 과정입니다.

## 후보 레포 1: CleanRL
- 링크: https://github.com/vwxyzjn/cleanrl
- 핵심 내용
    - PPO, DQN, SAC 등 주요 알고리즘이 복잡한 클래스 상속 없이 파일 하나에 전부 구현되어 있음
    - argparse를 통해 터미널에서 바로 하이퍼파라미터를 주입하거나, 코드 상단의 설정값을 바꾸면 즉시 반영됨
- 장점
    - poetry 또는 pip로 의존성 설치 후 python ppo.py 한 줄이면 바로 학습이 시작되어, 환경 세팅과 실행이 매우 간단함
    - 코드 수정 없이도 학습 곡선과 비디오를 웹 대시보드에서 실시간으로 확인 가능하여, 모델이 잘 학습되고 있는지 직관적으로 파악할 수 있음
    - 코드가 짧고 선형적이어서, 특정 수식을 수정했을 때 어디가 바뀌는지 직관적으로 파악할 수 있음
- 주의사항
    - 알고리즘마다 파일이 분리되어 있어, 공통 기능을 수정하려면 여러 파일을 일일이 고쳐야 하는 번거로움이 있음
    - 최신 알고리즘이 모두 구현되어 있지는 않으므로, 논문 구현 레포를 직접 찾아보는 것도 좋은 방법임

## 후보 레포 2: Stable-Baselines3 (SB3)
- 링크: https://github.com/DLR-RM/stable-baselines3
- 핵심 내용
    - PyTorch 기반의 검증된 RL 알고리즘 모음집
    - Scikit-learn과 유사한 인터페이스를 제공하여, 초보자도 학습 파이프라인을 쉽게 구축할 수 있음
- 장점
    - 기본적으로 TensorBoard 로그를 지원하며, EvalCallback을 사용하면 평가 주기에 따른 성능 변화를 그래프로 깔끔하게 뽑아낼 수 있음
    - Gymnasium 인터페이스만 맞추면 되므로, 각자 준비한 데이터를 환경으로 만들어 넣기 가장 수월함
    - 하이퍼파라미터 튜닝 프레임워크인 RL Zoo를 함께 제공하여, 최적의 파라미터를 찾는 방법을 배울 수 있음
- 주의사항
    - 알고리즘 내부 로직이 캡슐화되어 있어, 이를 뜯어고치려면 라이브러리 소스 코드를 깊게 파야 하는 번거로움이 있음

## 후보 레포 3: d3rlpy (Offline RL)
- 링크: https://github.com/takuseno/d3rlpy
- 핵심 내용
    - 오프라인 강화학습 특화 라이브러리로, 환경과의 상호작용 없이 미리 쌓아둔 데이터셋으로 학습
    - CQL, IQL 등 최신 알고리즘을 지원하며, 사용법이 직관적임
- 장점
    - 복잡한 시뮬레이터 구축 없이, CSV나 배열 형태의 데이터만 있으면 바로 학습 가능
    - 오프라인 데이터 대비 학습된 정책의 기대 보상 등을 로그로 잘 보여주어, 모델 개선 효과를 객관적으로 비교하기 좋음
    - 데이터셋 구조가 단순하여, 다양한 도메인의 데이터를 적용해보기 좋음
- 주의사항
    - 데이터가 충분하지 않거나 편향되어 있으면 학습이 전혀 되지 않을 수 있으므로, 데이터셋 준비에 신경 써야 함
    - 온라인 RL에 비해 알고리즘이 덜 직관적일 수 있으므로, 논문을 꼼꼼히 읽고 이해하는 과정이 필요할 수 있음
